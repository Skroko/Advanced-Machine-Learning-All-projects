# Advanced-Machine-Learning-All-projects

## Course description

The Advanced Machine Learning course focuses on three main areas: Generative Models, Geodesics and Latent Spaces, and Graph Machine Learning. Students learn to implement and evaluate Variational Autoencoders (VAEs) with different priors, and assess model performance through metrics like test set log-likelihood. They explore geometric properties of latent spaces, implementing algorithms for Fisher-Rao geodesics and analyzing their behavior. The course also covers graph machine learning, where students create generative models for graphs using techniques like message passing neural networks, and evaluate these models using metrics such as novelty, uniqueness, and graph statistics. These objectives aim to provide a comprehensive understanding of advanced machine learning techniques, both theoretical and practical.

# Overview

The course has three focus areas:
- Generative models
- Latent space analysis
- Graph neural networks

# Projects

## Generative Models

In this project, we aim to train and assess a variety of deep generative models on a binarized and a standard MNIST
dataset. We split the data into a training set (n=60,000) and a test set (n=10,000). Our project has two parts. In the first
part, we look at the impact of different priors on a Variational Autoencoder (VAE). In the second part, we train a flow
based model, and a DDPM. We compare the quality of the samples these models produce to each other and a VAE model.

<br/>

The different priors can be visualed as:

<img src="https://github.com/Skroko/Advanced-Machine-Learning-All-projects/blob/main/Imgs/withcbar.png" alt="P1T1" width="500"/>

Fig 1: Density plots of VAE priors with test data, color-
coded by label, after encoding to latent space. In the plots
the density is proportional to the brightness of the region

<br/>


<img src="https://github.com/Skroko/Advanced-Machine-Learning-All-projects/blob/main/Imgs/P1-T1.png" alt="P1T1" width="500"/>

Table 1: Test set metrics for VAEs using different priors

<br/>

<img src="https://github.com/Skroko/Advanced-Machine-Learning-All-projects/blob/main/Imgs/P1-T2.png" alt="P1T1" width="500"/>

Table 2: Quality metrics of different generative models

<br/>

<img src="https://github.com/Skroko/Advanced-Machine-Learning-All-projects/blob/main/Imgs/PartB_samples.png" alt="P1T1" width="500"/>

Fig 2: MNIST images generated by each of the three models, with real MNIST images for comparison

## Latent space analysis - Geodesics

In Mini-project 2 of Advanced Machine Learning (02460), we explored Fisher-Rao geodesics, ensemble VAE geometry, and the impact of initialization. We trained a VAE on a 3-class MNIST subset with a 2D latent space, using Fisher-Rao geodesics to measure distances between latent variables, optimized with LBFGS. Geodesics prioritized denser data regions but occasionally took shortcuts. To improve reliability, we repeated experiments with an ensemble decoder of 10 members, finding that geodesics followed more consistent paths and avoided shortcuts, with reliable proximity achieved with fewer ensemble members. Comparing linear initialization with Abstract Density Metric (ADM) initialization, we found ADM achieved lower initial Fisher-Rao energy and reduced optimization steps, enhancing efficiency. Overall, ensemble decoders and improved initialization strategies enhanced geodesic reliability and efficiency.

<img src="https://github.com/Skroko/Advanced-Machine-Learning-All-projects/blob/main/Imgs/P2F1.png" alt="P1T1" width="500"/>

<img src="https://github.com/Skroko/Advanced-Machine-Learning-All-projects/blob/main/Imgs/P2-T1F2.png" alt="P1T1" width="500"/>



## Graph neural networks

In Mini-project 3 of Advanced Machine Learning (02460), we aimed to synthesize new graphs similar to those in the MUTAG dataset using a graph-level Variational Autoencoder (VAE). We focused on adjacency matrices, utilizing spectral sorting for learning and comparing results against an Erdös-Rényi baseline. The VAE, built with Graph Convolutional Networks (GCNs) and trained using ELBO loss with a Mixture of Gaussians prior, generated more realistic graphs. Evaluations showed high novelty and uniqueness in generated graphs, though both VAE and baseline deviated from the empirical distribution. Quantitative analysis of graph statistics like node degree and clustering coefficient indicated that generated graphs were similar to the MUTAG dataset but differed slightly for higher values.

<img src="https://github.com/Skroko/Advanced-Machine-Learning-All-projects/blob/main/Imgs/P3-F1.png" alt="P1T1" width="500"/>
<img src="https://github.com/Skroko/Advanced-Machine-Learning-All-projects/blob/main/Imgs/P3-T1.png" alt="P1T1" width="500"/>
<img src="https://github.com/Skroko/Advanced-Machine-Learning-All-projects/blob/main/Imgs/P3-F2.png" alt="P1T1" width="500"/>
